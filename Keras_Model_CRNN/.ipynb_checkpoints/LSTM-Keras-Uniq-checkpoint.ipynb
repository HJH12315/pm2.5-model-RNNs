{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size: [6.0, 4.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lstm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, Panel\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import logistic\n",
    "%matplotlib inline\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "print (\"Current size:\", fig_size)\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 24\n",
    "fig_size[1] = 18\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "keras.backend.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, file, time_steps, scaler_type):\n",
    "        print (\"Loading data ...\")\n",
    "        \n",
    "#         print('Processing CSV :', file)\n",
    "#         data_temp = fill_gaps(data, \"SR\", 0.05)\n",
    "#         data_temp = fill_gaps(data_temp, \"WD\", 2.0)\n",
    "#         data_temp = fill_gaps(data_temp, \"WS\", 0.005)\n",
    "#         data_temp = fill_gaps(data_temp, \"VWS\", 0.005)\n",
    "#         data_temp = fill_gaps(data_temp, \"BP\", 0.1)\n",
    "#         data_temp = fill_gaps(data_temp, \"TEMP\", 0.05)\n",
    "#         data_temp = fill_gaps(data_temp, \"PM2.5\", 0.05)\n",
    "#         data_temp = fill_gaps(data_temp, \"RH\", 0.05)\n",
    "        \n",
    "        data_loaded = pd.read_pickle(file)\n",
    "        data_loaded.isnull().sum()\n",
    "        data_loaded_np = data_loaded[[\"PM2.5\",\"WS\",\"RH\",\"BP\",\"VWS\",\"SR\",\"WD\",\"TEMP\"]].as_matrix()\n",
    "        \n",
    "        self.time_steps = time_steps\n",
    "        self.scaler_type = scaler_type\n",
    "        self.X_norm_pm, self.y_norm_pm, self.scaler_pm, self.min_max_scaler_pm = self.generate_batch_data(data_loaded_np[0:,0], time_steps=self.time_steps, name=\"pm25\")\n",
    "        self.X_norm_ws, self.y_norm_ws, self.scaler_ws, self.min_max_scaler_ws = self.generate_batch_data(data_loaded_np[0:,1], time_steps=self.time_steps, name=\"ws\")\n",
    "        self.X_norm_rh, self.y_norm_rh, self.scaler_rh, self.min_max_scaler_rh = self.generate_batch_data(data_loaded_np[0:,2], time_steps=self.time_steps, name=\"rh\")\n",
    "        self.X_norm_bp, self.y_norm_bp, self.scaler_bp, self.min_max_scaler_bp = self.generate_batch_data(data_loaded_np[0:,4], time_steps=self.time_steps, name=\"bp\")\n",
    "        self.X_norm_vws, self.y_norm_vws, self.scaler_vws, self.min_max_scaler_vws = self.generate_batch_data(data_loaded_np[0:,3], time_steps=self.time_steps, name=\"vws\")\n",
    "        self.X_norm_sr, self.y_norm_sr, self.scaler_sr, self.min_max_scaler_sr = self.generate_batch_data(data_loaded_np[0:,5], time_steps=self.time_steps, name=\"sr\")\n",
    "        self.X_norm_wd, self.y_norm_wd, self.scaler_wd, self.min_max_scaler_wd = self.generate_batch_data(data_loaded_np[0:,6], time_steps=self.time_steps, name=\"wd\")\n",
    "        self.X_norm_temp, self.y_norm_temp, self.scaler_temp, self.min_max_scaler_temp = self.generate_batch_data(data_loaded_np[0:,7], time_steps=self.time_steps, name=\"temp\")\n",
    "\n",
    "        filename = \"np_\"+self.scaler_type+\"_process_comp_\"+str(self.time_steps)+\".npz\"\n",
    "        if os.path.isfile(filename):\n",
    "            print (\"Found existing file :\",filename)\n",
    "            print (\"Loading ...\")\n",
    "            npzfile = np.load(filename)\n",
    "            self.X_norm_pm = npzfile['arr_0']\n",
    "            self.X = npzfile['arr_1']\n",
    "            self.Y = npzfile['arr_2']\n",
    "            print (\"Complete.\")\n",
    "        else:\n",
    "            self.X = np.array(np.zeros([1, 7]))\n",
    "            for i in range(len(self.X_norm_pm)):\n",
    "                temp = np.column_stack((self.X_norm_ws[i],self.X_norm_rh[i],self.X_norm_bp[i],self.X_norm_vws[i],self.X_norm_sr[i],self.X_norm_wd[i],self.X_norm_temp[i]))\n",
    "                self.X = np.append(self.X, temp, axis=0)\n",
    "\n",
    "            self.X = self.X[1:].reshape(len(self.X_norm_pm),48,7)\n",
    "            self.Y = self.y_norm_pm\n",
    "            \n",
    "            print (\"Input shape :\",np.shape(self.X_norm_pm))\n",
    "            print (\"Aux Input shape :\",np.shape(self.X))\n",
    "            print (\"Output shape :\",np.shape(self.Y))\n",
    "            print (\"Saving file ...\")\n",
    "            np.savez(filename, self.X_norm_pm, self.X, self.Y)\n",
    "            print (\"Saved file to :\", filename)\n",
    "            print (\"Complete.\")\n",
    "        \n",
    "    def return_data(self):\n",
    "        return self.X_norm_pm, self.X, self.Y, self.scaler_pm, self.min_max_scaler_pm\n",
    "    \n",
    "    def fill_gaps(self, data, col, sigma):\n",
    "        temp = data.copy()\n",
    "        temp[\"FROM\"][0] = \"00:00:00\"\n",
    "\n",
    "        mu, sigma = 0, sigma \n",
    "        for k in range(len(temp)):\n",
    "            try:\n",
    "                if (str(temp[col][k]) == str(np.nan)):\n",
    "                    rolling_sum = 0\n",
    "                    noise = np.random.normal(mu, sigma, 1)[0]\n",
    "                    rolling_sum = rolling_sum + float(temp[col][k-24]) + float(temp[col][k-48]) + float(temp[col][k-72]) + float(temp[col][k-96]) + float(temp[col][k-120]) + float(temp[col][k-144]) + float(temp[col][k-168])\n",
    "                    temp[col][k] = round(1.0*rolling_sum/7 + noise,2)\n",
    "            except (IndexError, ValueError):\n",
    "                print (\"Break at index :\", k)\n",
    "                break\n",
    "        return temp\n",
    "\n",
    "    def shift(self, arr, num, fill_value=np.nan):\n",
    "        result = np.empty_like(arr)\n",
    "        if num > 0:\n",
    "            result[:num] = fill_value\n",
    "            result[num:] = arr[:-num]\n",
    "        elif num < 0:\n",
    "            result[num:] = fill_value\n",
    "            result[:num] = arr[-num:]\n",
    "        else:\n",
    "            result = arr\n",
    "        return result\n",
    "\n",
    "    def generate_batch_data(self, raw_data, time_steps, name):\n",
    "        series = pd.Series(raw_data, dtype=np.float32)\n",
    "        # prepare data for standardization\n",
    "        values = series.values\n",
    "        values = values.reshape((len(values), 1))\n",
    "\n",
    "        # train the standardization\n",
    "        if self.scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        if self.scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        if self.scaler_type == 'min_max':\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        if self.scaler_type == 'robust_min_max':\n",
    "            scaler = RobustScaler()\n",
    "        if self.scaler_type == 'standard_min_max':\n",
    "            scaler = StandardScaler()\n",
    "            \n",
    "        min_max_scaler = None    \n",
    "        scaler = scaler.fit(values)\n",
    "        print('feature ------------ ', name.upper())\n",
    "        \n",
    "        if self.scaler_type == 'standard':\n",
    "            print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, math.sqrt(scaler.var_)))\n",
    "            values[values > 450] = 3*scaler.mean_\n",
    "            print('Data normalized... Replaced the outliers with 3 times the mean value')\n",
    "        if self.scaler_type == 'robust':\n",
    "            print('Data normalized... Using Robust Scaling')\n",
    "        if self.scaler_type == 'min_max':\n",
    "            print('Data normalized... Using Min-Max Scaling')\n",
    "       \n",
    "        normalized = scaler.transform(values)\n",
    "        \n",
    "        # min_max scaling\n",
    "        if self.scaler_type == 'robust_min_max' or self.scaler_type == 'standard_min_max':\n",
    "            min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            min_max_scaler = min_max_scaler.fit(normalized)\n",
    "            normalized = min_max_scaler.transform(normalized)\n",
    "            if self.scaler_type == 'robust_min_max':\n",
    "                print('Data normalized... Using Robust_Min-Max Scaling')\n",
    "            if self.scaler_type == 'standard_min_max':\n",
    "                print('Data normalized... Using Standard_Min-Max Scaling')\n",
    "            \n",
    "        # batch formation\n",
    "        data = normalized\n",
    "        print('Max: %f, Min: %f' % (np.amax(data), np.amin(data)))\n",
    "        x = data[:(len(data)-(len(data) % time_steps))]\n",
    "        y = self.shift(data,-(time_steps)).astype(np.float32)\n",
    "\n",
    "        x_batches = np.array([])\n",
    "        y_batches = np.array([])\n",
    "\n",
    "        # check if file exists\n",
    "        seq_file_name = \"np_\"+self.scaler_type+\"_processed_\"+name+\"_\"+str(time_steps)+\".npz\"\n",
    "        if os.path.isfile(seq_file_name):\n",
    "            npzfile = np.load(seq_file_name)\n",
    "            x_batches = npzfile['arr_0']\n",
    "            y_batches = npzfile['arr_1']\n",
    "            return x_batches, y_batches, scaler, min_max_scaler\n",
    "        else: \n",
    "            for i in range(len(y)):\n",
    "                try:\n",
    "                    x_batches = np.append(x_batches, x[i:i+time_steps].reshape(-1,12,1))\n",
    "                    y_batches = np.append(y_batches, y[i].reshape(-1))\n",
    "                except ValueError:\n",
    "                    break\n",
    "            x_batches = x_batches.reshape(-1, time_steps, 1)\n",
    "            y_batches = y_batches.reshape(-1)\n",
    "            np.savez(seq_file_name, x_batches, y_batches)\n",
    "            return x_batches, y_batches, scaler, min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "feature ------------  PM25\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "feature ------------  WS\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "feature ------------  RH\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "feature ------------  BP\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "feature ------------  VWS\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "feature ------------  SR\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "feature ------------  WD\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "feature ------------  TEMP\n",
      "Data normalized... Using Standard_Min-Max Scaling\n",
      "Max: 1.000000, Min: -1.000000\n",
      "Found existing file : np_standard_min_max_process_comp_48.npz\n",
      "Loading ...\n",
      "Complete.\n",
      "> Data Loaded. Compiling...\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    global_start_time = time.time()\n",
    "    startTime = datetime.now() \n",
    "    epochs  = 100\n",
    "    state_neurons_1 = 512\n",
    "    state_neurons_2 = 512\n",
    "    output = 1\n",
    "    seed = 7\n",
    "    batch_size = 32\n",
    "    scaler_type=\"min_max\"\n",
    "#     dropouts = [0.05, 0.14, 0.35]\n",
    "    dropouts = [0.05, 0.25]\n",
    "    seq_len = 48\n",
    "    \n",
    "    \n",
    "    batch_generator_obj = BatchGenerator(file=\"mumbai_6.pkl\", time_steps=seq_len, scaler_type=scaler_type)\n",
    "    X_norm, X_aux_norm, y_norm, scaler, min_max_scaler = batch_generator_obj.return_data()\n",
    "    X_train, X_val, X_aux_train, X_aux_val, y_train, y_val = train_test_split(X_norm, X_aux_norm, y_norm, test_size=0.25, random_state=seed)\n",
    "    X_val, X_test, X_aux_val, X_aux_test, y_val, y_test = train_test_split(X_val, X_aux_val, y_val, test_size=0.50, random_state=seed)\n",
    "    \n",
    "    print('> Data Loaded. Compiling...')\n",
    "    model = lstm.build_model([4, 7], [7, seq_len, state_neurons_1, state_neurons_2, output], dropouts)\n",
    "    mod_hist = model.fit([X_train, X_aux_train], [y_train], validation_data=([X_val, X_aux_val], [y_val]), epochs=epochs, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print('Training duration (s) : ', time.time() - global_start_time)\n",
    "    print('Training duration (Hr) : ', datetime.now() - startTime)\n",
    "    \n",
    "    file_name = \"LSTM_Keras_multi_tanh_adam_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_2)+\"_\"+str(batch_size)\n",
    "    print(\"Saving model : \"+file_name+\".h5\")\n",
    "    model.save(file_name+\".h5\")\n",
    "    print(\"Model saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #predict sequence\n",
    "    _y_norm = lstm.predict_point_by_point_aux(model, [X_norm, X_aux_norm])\n",
    "    _y_train = lstm.predict_point_by_point_aux(model, [X_train, X_aux_train])\n",
    "    _y_val = lstm.predict_point_by_point_aux(model, [X_val, X_aux_val])\n",
    "    _y_test = lstm.predict_point_by_point_aux(model,  [X_test, X_aux_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    _y_norm = _y_norm.reshape(-1,1)\n",
    "    _y_train = _y_train.reshape(-1,1)\n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "    _y_test = _y_test.reshape(-1,1)\n",
    "    \n",
    "    y_norm = y_norm.reshape(-1,1)\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    y_test = y_test.reshape(-1,1)\n",
    "    \n",
    "    print(\"Predicted Output shape: \", np.shape(_y_norm))\n",
    "    print(\"Original Output shape:  \", np.shape(y_norm))\n",
    "    \n",
    "    if not (min_max_scaler is None):\n",
    "        print(\"Using Min_Max Inversion ...\")\n",
    "        _y_norm = min_max_scaler.inverse_transform(_y_norm)\n",
    "        _y_train = min_max_scaler.inverse_transform(_y_train)\n",
    "        _y_val = min_max_scaler.inverse_transform(_y_val)\n",
    "        _y_test = min_max_scaler.inverse_transform(_y_test)\n",
    "        \n",
    "        y_norm = min_max_scaler.inverse_transform(y_norm)\n",
    "        y_train = min_max_scaler.inverse_transform(y_train)\n",
    "        y_val = min_max_scaler.inverse_transform(y_val)\n",
    "        y_test = min_max_scaler.inverse_transform(y_test)\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y = scaler.inverse_transform(_y_norm)\n",
    "    _y_tr = scaler.inverse_transform(_y_train)\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "    _y_te = scaler.inverse_transform(_y_test)\n",
    "    for i in range(5):\n",
    "        print(_y[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y = scaler.inverse_transform(y_norm)\n",
    "    y_tr = scaler.inverse_transform(y_train)\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    y_te = scaler.inverse_transform(y_test)\n",
    "    for i in range(5):\n",
    "        print(y[i])    \n",
    "    \n",
    "    #predicted\n",
    "    _Y = pd.Series(np.ravel(_y))\n",
    "    _Y_TR = pd.Series(np.ravel(_y_tr))\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "    _Y_TE = pd.Series(np.ravel(_y_te))\n",
    "    \n",
    "    #original\n",
    "    Y =  pd.Series(np.ravel(y))\n",
    "    Y_TR = pd.Series(np.ravel(y_tr))\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "    Y_TE = pd.Series(np.ravel(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    print('Training RMSE :', math.sqrt(mean_squared_error(_Y_TR, Y_TR)))\n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "    print('Test RMSE :', math.sqrt(mean_squared_error(_Y_TE, Y_TE)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_TE[100:200], label='predicted')\n",
    "    plot_train, = plt.plot(Y_TE[100:200], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(mod_hist.history['loss'])\n",
    "plt.plot(mod_hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(inputs):\n",
    "    \"\"\"\n",
    "    Calculate the softmax for the give inputs (array)\n",
    "    :param inputs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.exp(inputs) / float(sum(np.exp(inputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(x):\n",
    "    return float(1.0 * x/(x+10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
