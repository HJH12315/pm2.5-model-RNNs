{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size: [6.0, 4.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lstm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, Panel\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import logistic\n",
    "%matplotlib inline\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "print (\"Current size:\", fig_size)\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 24\n",
    "fig_size[1] = 18\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "keras.backend.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, file, time_steps=48, scaler_type='standard', pca=False, pca_dim=8, normal=False):\n",
    "        self.file = file\n",
    "        self.time_steps = time_steps\n",
    "        self.scaler_type = scaler_type\n",
    "        self.pca = pca\n",
    "        self.pca_dim = pca_dim\n",
    "        self.normal = normal\n",
    "        \n",
    "        print (\"Loading data ...\")\n",
    "        data_loaded = pd.read_pickle(self.file)\n",
    "        data_loaded.isnull().sum()\n",
    "        \n",
    "        # load data\n",
    "        data_loaded_np = data_loaded[[\"PM2.5\",\"WS\",\"RH\",\"BP\",\"VWS\",\"SR\",\"WD\",\"TEMP\"]].as_matrix()\n",
    "       \n",
    "        # PCA\n",
    "        if self.pca == True:\n",
    "            print(\"PCA transform\")\n",
    "            pca = PCA(n_components=self.pca_dim, svd_solver='full')\n",
    "            pca = pca.fit(data_loaded_np)\n",
    "            data_loaded_np = pca.transform(data_loaded_np)\n",
    "        \n",
    "        #Row Normalization    \n",
    "        if self.normal == True:\n",
    "            print(\"Normalize transform\")\n",
    "            self.norm_scaler = Normalizer().fit(data_loaded_np)\n",
    "            data_loaded_np = self.norm_scaler.transform(data_loaded_np)\n",
    "\n",
    "        self.X_norm_pm, self.y_norm_pm, self.scaler_pm = self.generate_batch_data(data_loaded_np[0:,0], time_steps=self.time_steps, name=\"pm25\")\n",
    "        self.X_norm_ws, self.y_norm_ws, self.scaler_ws = self.generate_batch_data(data_loaded_np[0:,1], time_steps=self.time_steps, name=\"ws\")\n",
    "        self.X_norm_rh, self.y_norm_rh, self.scaler_rh = self.generate_batch_data(data_loaded_np[0:,2], time_steps=self.time_steps, name=\"rh\")\n",
    "        self.X_norm_bp, self.y_norm_bp, self.scaler_bp = self.generate_batch_data(data_loaded_np[0:,3], time_steps=self.time_steps, name=\"bp\")\n",
    "        self.X_norm_vws, self.y_norm_vws, self.scaler_vws = self.generate_batch_data(data_loaded_np[0:,4], time_steps=self.time_steps, name=\"vws\")\n",
    "        self.X_norm_sr, self.y_norm_sr, self.scaler_sr = self.generate_batch_data(data_loaded_np[0:,5], time_steps=self.time_steps, name=\"sr\")\n",
    "        self.X_norm_wd, self.y_norm_wd, self.scaler_wd = self.generate_batch_data(data_loaded_np[0:,6], time_steps=self.time_steps, name=\"wd\")\n",
    "        self.X_norm_temp, self.y_norm_temp, self.scaler_temp = self.generate_batch_data(data_loaded_np[0:,7], time_steps=self.time_steps, name=\"temp\")\n",
    "\n",
    "        if not (self.scaler_type is None):\n",
    "            filename = \"np_\"+self.scaler_type+\"_process_comp_\"+str(self.time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "        else:\n",
    "            filename = \"np_process_comp_\"+str(self.time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "\n",
    "        if os.path.isfile(\"data_log/\"+filename):\n",
    "            print (\"Found existing file :\",\"data_log/\"+filename)\n",
    "            print (\"Loading ...\")\n",
    "            npzfile = np.load(\"data_log/\"+filename)\n",
    "            self.X_norm_pm = npzfile['arr_0']\n",
    "            self.X_norm_ws = npzfile['arr_1']\n",
    "            self.X_norm_rh = npzfile['arr_2']\n",
    "            self.X_norm_bp = npzfile['arr_3']\n",
    "            self.X_norm_vws = npzfile['arr_4']\n",
    "            self.X_norm_sr = npzfile['arr_5']\n",
    "            self.X_norm_wd = npzfile['arr_6']\n",
    "            self.X_norm_temp = npzfile['arr_7']\n",
    "            self.Y = npzfile['arr_8']\n",
    "            print (\"Complete.\")\n",
    "        else:\n",
    "            self.Y = self.y_norm_pm \n",
    "            print (\"Input shape pm25:\",np.shape(self.X_norm_pm))\n",
    "            print (\"Input shape ws:\",np.shape(self.X_norm_ws))\n",
    "            print (\"Input shape rh:\",np.shape(self.X_norm_rh))\n",
    "            print (\"Input shape bp:\",np.shape(self.X_norm_bp))\n",
    "            print (\"Input shape vws:\",np.shape(self.X_norm_vws))\n",
    "            print (\"Input shape sr:\",np.shape(self.X_norm_sr))\n",
    "            print (\"Input shape wd:\",np.shape(self.X_norm_wd))\n",
    "            print (\"Input shape temp:\",np.shape(self.X_norm_temp))\n",
    "            print (\"Output shape :\",np.shape(self.Y))\n",
    "            print (\"Saving file ...\")\n",
    "            np.savez(\"data_log/\"+filename, self.X_norm_pm, self.X_norm_ws, self.X_norm_rh, self.X_norm_bp, self.X_norm_vws, self.X_norm_sr, self.X_norm_wd, self.X_norm_temp, self.Y)\n",
    "            print (\"Saved file to :\", filename)\n",
    "            print (\"Complete.\")\n",
    "        \n",
    "    def return_data(self):\n",
    "        return self.X_norm_pm, self.X_norm_ws, self.X_norm_rh, self.X_norm_bp, self.X_norm_vws, self.X_norm_sr, self.X_norm_wd, self.X_norm_temp, self.Y, self.scaler_pm\n",
    "\n",
    "    def shift(self, arr, num, fill_value=np.nan):\n",
    "        result = np.empty_like(arr)\n",
    "        if num > 0:\n",
    "            result[:num] = fill_value\n",
    "            result[num:] = arr[:-num]\n",
    "        elif num < 0:\n",
    "            result[num:] = fill_value\n",
    "            result[:num] = arr[-num:]\n",
    "        else:\n",
    "            result = arr\n",
    "        return result\n",
    "\n",
    "    def generate_batch_data(self, raw_data, time_steps, name):\n",
    "        series = pd.Series(raw_data, dtype=np.float32)\n",
    "        values = series.values\n",
    "        values = values.reshape((len(values), 1))\n",
    "        print('feature ------------ ', name.upper())\n",
    "        \n",
    "        if self.scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        if self.scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        if self.scaler_type == 'min_max':\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            \n",
    "        scaler = scaler.fit(values)\n",
    "        normalized = scaler.transform(values)\n",
    "        \n",
    "        #data = values\n",
    "        data = normalized\n",
    "        print('Max: %f, Min: %f' % (np.amax(data), np.amin(data)))\n",
    "        x = data[:(len(data)-(len(data) % time_steps))]\n",
    "        y = self.shift(data,-(time_steps)).astype(np.float32)\n",
    "\n",
    "        x_batches = np.array([])\n",
    "        y_batches = np.array([])\n",
    "\n",
    "        # check if file exists\n",
    "        if (self.scaler_type is None):\n",
    "            seq_file_name = \"np_processed_\"+name+\"_\"+str(time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "        else:\n",
    "            seq_file_name = \"np_\"+self.scaler_type+\"_processed_\"+name+\"_\"+str(time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"          \n",
    "\n",
    "        if os.path.isfile(\"data_log/\"+seq_file_name):\n",
    "            npzfile = np.load(\"data_log/\"+seq_file_name)\n",
    "            x_batches = npzfile['arr_0']\n",
    "            y_batches = npzfile['arr_1']\n",
    "            return x_batches, y_batches, scaler\n",
    "        else: \n",
    "            for i in range(len(y)):\n",
    "                try:\n",
    "                    x_batches = np.append(x_batches, x[i:i+(time_steps)].reshape(-1,12,1))\n",
    "                    y_batches = np.append(y_batches, y[i].reshape(-1))\n",
    "                except ValueError:\n",
    "                    break\n",
    "                    \n",
    "            x_batches = x_batches.reshape(-1, time_steps, 1)\n",
    "            y_batches = y_batches.reshape(-1)\n",
    "            np.savez(\"data_log/\"+seq_file_name, x_batches, y_batches)\n",
    "            return x_batches, y_batches, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-ff9be614a4e4>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-ff9be614a4e4>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    = train_test_split(X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp, y_norm, test_size=0.20, random_state=seed)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    global_start_time = time.time()\n",
    "    startTime = datetime.now() \n",
    "    epochs  = 150\n",
    "    state_neurons_1 = 64\n",
    "    state_neurons_2 = 64\n",
    "    state_neurons_3 = 64\n",
    "    output = 1\n",
    "    seed = 7\n",
    "    batch_size = 32\n",
    "    scaler_type=\"standard\"\n",
    "    dropouts = [0.15, 0.25, 0.45]\n",
    "    seq_len = 48\n",
    "    \n",
    "    \n",
    "    batch_generator_obj = BatchGenerator(file=\"data_log/mumbai_6_log_pro.pkl\", time_steps=48, scaler_type=scaler_type, pca=False, pca_dim=8, normal=False)\n",
    "    X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp, y_norm, scaler = batch_generator_obj.return_data()\n",
    "    \n",
    "    X_train_pm, X_val_pm, X_train_ws, X_val_ws, X_train_rh, X_val_rh, X_train_bp, X_val_bp, X_train_vws, X_val_vws, X_train_sr, X_val_sr, X_train_wd, X_val_wd, X_train_temp, X_val_temp, y_train, y_val = train_test_split(X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp, y_norm, test_size=0.20, random_state=seed)\n",
    "    \n",
    "    print('> Data Loaded. Compiling...')\n",
    "    model = lstm.build_model([4, 7], [7, seq_len, state_neurons_1, state_neurons_2, output], dropouts)\n",
    "    \n",
    "    file_name = \"wt_LSTM_Keras_tanh_rmsprop_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    checkpoint = ModelCheckpoint(file_name+\".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    mod_hist = model.fit([X_train_pm, X_train_ws, X_train_rh, X_train_bp, X_train_vws, X_train_sr, X_train_wd, X_train_temp], [y_train], validation_data=([X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp], [y_val]), epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, shuffle=True)\n",
    "\n",
    "    print('Training duration (s) : ', time.time() - global_start_time)\n",
    "    print('Training duration (Hr) : ', datetime.now() - startTime)\n",
    "    \n",
    "    file = \"LSTM_Keras_linear_rmsprop_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    print(\"Saving model : \"+file+\".h5\")\n",
    "    model.save(file+\".h5\")\n",
    "    print(\"Model saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #predict sequence\n",
    "    _y_norm = lstm.predict_point_by_point_aux(model, [X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp])\n",
    "    _y_train = lstm.predict_point_by_point_aux(model, [X_train_pm, X_train_ws, X_train_rh, X_train_bp, X_train_vws, X_train_sr, X_train_wd, X_train_temp])\n",
    "    _y_val = lstm.predict_point_by_point_aux(model, [X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    _y_norm = _y_norm.reshape(-1,1)\n",
    "    _y_train = _y_train.reshape(-1,1)\n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "    \n",
    "    y_norm = y_norm.reshape(-1,1)\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    \n",
    "    print(\"Predicted Output shape: \", np.shape(_y_norm))\n",
    "    print(\"Original Output shape:  \", np.shape(y_norm))\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y = scaler.inverse_transform(_y_norm)\n",
    "    _y_tr = scaler.inverse_transform(_y_train)\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "    for i in range(5):\n",
    "        print(_y_va[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y = scaler.inverse_transform(y_norm)\n",
    "    y_tr = scaler.inverse_transform(y_train)\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    for i in range(5):\n",
    "        print(y_va[i])    \n",
    "    \n",
    "    #predicted\n",
    "    _Y = pd.Series(np.ravel(_y))\n",
    "    _Y_TR = pd.Series(np.ravel(_y_tr))\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "#     _Y_TE = pd.Series(np.ravel(_y_te))\n",
    "    \n",
    "    #original\n",
    "    Y =  pd.Series(np.ravel(y))\n",
    "    Y_TR = pd.Series(np.ravel(y_tr))\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "#     Y_TE = pd.Series(np.ravel(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    print('Total RMSE :', math.sqrt(mean_squared_error(_Y, Y)))\n",
    "    print('Training RMSE :', math.sqrt(mean_squared_error(_Y_TR, Y_TR)))\n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "#     print('Test RMSE :', math.sqrt(mean_squared_error(_Y_TE, Y_TE)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_VA[100:300], label='predicted')\n",
    "    plot_train, = plt.plot(Y_VA[100:300], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(mod_hist.history['loss'])\n",
    "plt.plot(mod_hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #recreate model \n",
    "    model = lstm.build_model([4, 7], [7, seq_len, state_neurons_1, state_neurons_2, output], dropouts, pre_train=file_name+\".h5\")\n",
    "    print(\"Created model and loaded weights from \"+file_name+\".h5\")\n",
    "    \n",
    "    # estimate accuracy on whole dataset using loaded weights\n",
    "    _y_val = lstm.predict_point_by_point_aux(model, [X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp])\n",
    "    \n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "    \n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "    for i in range(5):\n",
    "        print(_y_va[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    for i in range(5):\n",
    "        print(y_va[i])    \n",
    "    \n",
    "    \n",
    "    y_va = np.exp(y_va)\n",
    "    _y_va = np.exp(_y_va)\n",
    "    \n",
    "    #predicted\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "    \n",
    "    #original\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "    \n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_VA[100:300], label='predicted')\n",
    "    plot_train, = plt.plot(Y_VA[100:300], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(x):\n",
    "    return float(1.0 * x/(x+10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
