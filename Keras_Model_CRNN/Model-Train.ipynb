{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size: [6.0, 4.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lstm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, Panel\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import logistic\n",
    "%matplotlib inline\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "print (\"Current size:\", fig_size)\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 24\n",
    "fig_size[1] = 18\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "keras.backend.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, file, time_steps=48, scaler_type='standard', pca=False, pca_dim=8, normal=False):\n",
    "        self.file = file\n",
    "        self.time_steps = time_steps\n",
    "        self.scaler_type = scaler_type\n",
    "        self.pca = pca\n",
    "        self.pca_dim = pca_dim\n",
    "        self.normal = normal\n",
    "        \n",
    "        print (\"Loading data ...\")\n",
    "        data_loaded = pd.read_pickle(self.file)\n",
    "        data_loaded.isnull().sum()\n",
    "        \n",
    "        # load data\n",
    "        data_loaded_np = data_loaded[[\"PM2.5\",\"WS\",\"RH\",\"BP\",\"VWS\",\"SR\",\"WD\",\"TEMP\"]].as_matrix()\n",
    "       \n",
    "        # PCA\n",
    "        if self.pca == True:\n",
    "            print(\"PCA transform\")\n",
    "            pca = PCA(n_components=self.pca_dim, svd_solver='full')\n",
    "            pca = pca.fit(data_loaded_np)\n",
    "            data_loaded_np = pca.transform(data_loaded_np)\n",
    "        \n",
    "        #Row Normalization    \n",
    "        if self.normal == True:\n",
    "            print(\"Normalize transform\")\n",
    "            self.norm_scaler = Normalizer().fit(data_loaded_np)\n",
    "            data_loaded_np = self.norm_scaler.transform(data_loaded_np)\n",
    "\n",
    "        self.X_norm_pm, self.y_norm_pm, self.scaler_pm = self.generate_batch_data(data_loaded_np[0:,0], time_steps=self.time_steps, name=\"pm25\")\n",
    "        self.X_norm_ws, self.y_norm_ws, self.scaler_ws = self.generate_batch_data(data_loaded_np[0:,1], time_steps=self.time_steps, name=\"ws\")\n",
    "        self.X_norm_rh, self.y_norm_rh, self.scaler_rh = self.generate_batch_data(data_loaded_np[0:,2], time_steps=self.time_steps, name=\"rh\")\n",
    "        self.X_norm_bp, self.y_norm_bp, self.scaler_bp = self.generate_batch_data(data_loaded_np[0:,3], time_steps=self.time_steps, name=\"bp\")\n",
    "        self.X_norm_vws, self.y_norm_vws, self.scaler_vws = self.generate_batch_data(data_loaded_np[0:,4], time_steps=self.time_steps, name=\"vws\")\n",
    "        self.X_norm_sr, self.y_norm_sr, self.scaler_sr = self.generate_batch_data(data_loaded_np[0:,5], time_steps=self.time_steps, name=\"sr\")\n",
    "        self.X_norm_wd, self.y_norm_wd, self.scaler_wd = self.generate_batch_data(data_loaded_np[0:,6], time_steps=self.time_steps, name=\"wd\")\n",
    "        self.X_norm_temp, self.y_norm_temp, self.scaler_temp = self.generate_batch_data(data_loaded_np[0:,7], time_steps=self.time_steps, name=\"temp\")\n",
    "\n",
    "        if not (self.scaler_type is None):\n",
    "            filename = \"np_\"+self.scaler_type+\"_process_comp_\"+str(self.time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "        else:\n",
    "            filename = \"np_process_comp_\"+str(self.time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "\n",
    "        if os.path.isfile(\"data_log/\"+filename):\n",
    "            print (\"Found existing file :\",\"data_log/\"+filename)\n",
    "            print (\"Loading ...\")\n",
    "            npzfile = np.load(\"data_log/\"+filename)\n",
    "            self.X_norm_pm = npzfile['arr_0']\n",
    "            self.X_norm_ws = npzfile['arr_1']\n",
    "            self.X_norm_rh = npzfile['arr_2']\n",
    "            self.X_norm_bp = npzfile['arr_3']\n",
    "            self.X_norm_vws = npzfile['arr_4']\n",
    "            self.X_norm_sr = npzfile['arr_5']\n",
    "            self.X_norm_wd = npzfile['arr_6']\n",
    "            self.X_norm_temp = npzfile['arr_7']\n",
    "            self.Y = npzfile['arr_8']\n",
    "            print (\"Complete.\")\n",
    "        else:\n",
    "            self.Y = self.y_norm_pm \n",
    "            print (\"Input shape pm25:\",np.shape(self.X_norm_pm))\n",
    "            print (\"Input shape ws:\",np.shape(self.X_norm_ws))\n",
    "            print (\"Input shape rh:\",np.shape(self.X_norm_rh))\n",
    "            print (\"Input shape bp:\",np.shape(self.X_norm_bp))\n",
    "            print (\"Input shape vws:\",np.shape(self.X_norm_vws))\n",
    "            print (\"Input shape sr:\",np.shape(self.X_norm_sr))\n",
    "            print (\"Input shape wd:\",np.shape(self.X_norm_wd))\n",
    "            print (\"Input shape temp:\",np.shape(self.X_norm_temp))\n",
    "            print (\"Output shape :\",np.shape(self.Y))\n",
    "            print (\"Saving file ...\")\n",
    "            np.savez(\"data_log/\"+filename, self.X_norm_pm, self.X_norm_ws, self.X_norm_rh, self.X_norm_bp, self.X_norm_vws, self.X_norm_sr, self.X_norm_wd, self.X_norm_temp, self.Y)\n",
    "            print (\"Saved file to :\", filename)\n",
    "            print (\"Complete.\")\n",
    "        \n",
    "    def return_data(self):\n",
    "        return self.X_norm_pm, self.X_norm_ws, self.X_norm_rh, self.X_norm_bp, self.X_norm_vws, self.X_norm_sr, self.X_norm_wd, self.X_norm_temp, self.Y, self.scaler_pm\n",
    "\n",
    "    def shift(self, arr, num, fill_value=np.nan):\n",
    "        result = np.empty_like(arr)\n",
    "        if num > 0:\n",
    "            result[:num] = fill_value\n",
    "            result[num:] = arr[:-num]\n",
    "        elif num < 0:\n",
    "            result[num:] = fill_value\n",
    "            result[:num] = arr[-num:]\n",
    "        else:\n",
    "            result = arr\n",
    "        return result\n",
    "\n",
    "    def generate_batch_data(self, raw_data, time_steps, name):\n",
    "        series = pd.Series(raw_data, dtype=np.float32)\n",
    "        values = series.values\n",
    "        values = values.reshape((len(values), 1))\n",
    "        print('feature ------------ ', name.upper())\n",
    "        \n",
    "        if self.scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        if self.scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        if self.scaler_type == 'min_max':\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            \n",
    "        scaler = scaler.fit(values)\n",
    "        normalized = scaler.transform(values)\n",
    "        \n",
    "        #data = values\n",
    "        data = normalized\n",
    "        print('Max: %f, Min: %f' % (np.amax(data), np.amin(data)))\n",
    "        x = data[:(len(data)-(len(data) % time_steps))]\n",
    "        y = self.shift(data,-(time_steps)).astype(np.float32)\n",
    "\n",
    "        x_batches = np.array([])\n",
    "        y_batches = np.array([])\n",
    "\n",
    "        # check if file exists\n",
    "        if (self.scaler_type is None):\n",
    "            seq_file_name = \"np_processed_\"+name+\"_\"+str(time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "        else:\n",
    "            seq_file_name = \"np_\"+self.scaler_type+\"_processed_\"+name+\"_\"+str(time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"          \n",
    "\n",
    "        if os.path.isfile(\"data_log/\"+seq_file_name):\n",
    "            npzfile = np.load(\"data_log/\"+seq_file_name)\n",
    "            x_batches = npzfile['arr_0']\n",
    "            y_batches = npzfile['arr_1']\n",
    "            return x_batches, y_batches, scaler\n",
    "        else: \n",
    "            for i in range(len(y)):\n",
    "                try:\n",
    "                    x_batches = np.append(x_batches, x[i:i+(time_steps)].reshape(-1,12,1))\n",
    "                    y_batches = np.append(y_batches, y[i].reshape(-1))\n",
    "                except ValueError:\n",
    "                    break\n",
    "                    \n",
    "            x_batches = x_batches.reshape(-1, time_steps, 1)\n",
    "            y_batches = y_batches.reshape(-1)\n",
    "            np.savez(\"data_log/\"+seq_file_name, x_batches, y_batches)\n",
    "            return x_batches, y_batches, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "feature ------------  PM25\n",
      "Max: 3.284291, Min: -10.752898\n",
      "feature ------------  WS\n",
      "Max: 3.415116, Min: -2.449860\n",
      "feature ------------  RH\n",
      "Max: 1.363076, Min: -4.063657\n",
      "feature ------------  BP\n",
      "Max: 5.360242, Min: -7.499539\n",
      "feature ------------  VWS\n",
      "Max: 9.401879, Min: -11.963299\n",
      "feature ------------  SR\n",
      "Max: 1.637784, Min: -2.654759\n",
      "feature ------------  WD\n",
      "Max: 1.076242, Min: -6.484889\n",
      "feature ------------  TEMP\n",
      "Max: 3.006956, Min: -3.982146\n",
      "Found existing file : data_log/np_standard_process_comp_48_False_False.npz\n",
      "Loading ...\n",
      "Complete.\n",
      "> Data Loaded. Compiling...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 41, 128)      1152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 41, 128)      1152        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 41, 128)      1152        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 41, 128)      1152        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 41, 128)      1152        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 41, 128)      1152        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 41, 128)      1152        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 41, 128)      1152        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 41, 128)      512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 41, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 41, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 41, 128)      512         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 41, 128)      512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 41, 128)      512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 41, 128)      512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 41, 128)      512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 41, 1024)     0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)          (None, 41, 256)      984576      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 41, 256)      0           cu_dnngru_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)          (None, 41, 256)      394752      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 41, 256)      0           cu_dnngru_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 41, 256)      65792       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 41, 256)      1024        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 41, 1)        257         batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 41, 1)        4           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 41)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            42          flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,459,759\n",
      "Trainable params: 1,457,197\n",
      "Non-trainable params: 2,562\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "> Compilation Time :  0.033087968826293945\n",
      "Train on 15706 samples, validate on 3927 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.5223Epoch 00000: val_loss improved from inf to 0.44777, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.5217 - val_loss: 0.4478\n",
      "Epoch 2/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.4431Epoch 00001: val_loss improved from 0.44777 to 0.40108, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.4433 - val_loss: 0.4011\n",
      "Epoch 3/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.4038Epoch 00002: val_loss improved from 0.40108 to 0.38303, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.4039 - val_loss: 0.3830\n",
      "Epoch 4/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.3747Epoch 00003: val_loss improved from 0.38303 to 0.36894, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3739 - val_loss: 0.3689\n",
      "Epoch 5/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.3493Epoch 00004: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3495 - val_loss: 0.3742\n",
      "Epoch 6/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.3357Epoch 00005: val_loss improved from 0.36894 to 0.36647, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3355 - val_loss: 0.3665\n",
      "Epoch 7/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.3199Epoch 00006: val_loss improved from 0.36647 to 0.34784, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3199 - val_loss: 0.3478\n",
      "Epoch 8/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.3065Epoch 00007: val_loss improved from 0.34784 to 0.33436, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3063 - val_loss: 0.3344\n",
      "Epoch 9/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2992Epoch 00008: val_loss improved from 0.33436 to 0.32633, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2989 - val_loss: 0.3263\n",
      "Epoch 10/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2865Epoch 00009: val_loss improved from 0.32633 to 0.31904, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2877 - val_loss: 0.3190\n",
      "Epoch 11/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2842Epoch 00010: val_loss improved from 0.31904 to 0.31518, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2836 - val_loss: 0.3152\n",
      "Epoch 12/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2700Epoch 00011: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2699 - val_loss: 0.3162\n",
      "Epoch 13/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2662Epoch 00012: val_loss improved from 0.31518 to 0.30908, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2667 - val_loss: 0.3091\n",
      "Epoch 14/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2568Epoch 00013: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2567 - val_loss: 0.3115\n",
      "Epoch 15/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2482Epoch 00014: val_loss improved from 0.30908 to 0.30163, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2479 - val_loss: 0.3016\n",
      "Epoch 16/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2446Epoch 00015: val_loss improved from 0.30163 to 0.29621, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2446 - val_loss: 0.2962\n",
      "Epoch 17/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2386Epoch 00016: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2384 - val_loss: 0.2974\n",
      "Epoch 18/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.2339Epoch 00017: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2338 - val_loss: 0.2978\n",
      "Epoch 19/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2218Epoch 00018: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2228 - val_loss: 0.3078\n",
      "Epoch 20/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2169Epoch 00019: val_loss improved from 0.29621 to 0.29309, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2167 - val_loss: 0.2931\n",
      "Epoch 21/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2150Epoch 00020: val_loss improved from 0.29309 to 0.29122, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2146 - val_loss: 0.2912\n",
      "Epoch 22/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2097Epoch 00021: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2112 - val_loss: 0.2913\n",
      "Epoch 23/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.2049Epoch 00022: val_loss improved from 0.29122 to 0.29120, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2046 - val_loss: 0.2912\n",
      "Epoch 24/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1999Epoch 00023: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2006 - val_loss: 0.2992\n",
      "Epoch 25/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1936Epoch 00024: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1936 - val_loss: 0.3037\n",
      "Epoch 26/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1881Epoch 00025: val_loss improved from 0.29120 to 0.28576, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1883 - val_loss: 0.2858\n",
      "Epoch 27/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1894Epoch 00026: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1892 - val_loss: 0.2999\n",
      "Epoch 28/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1785Epoch 00027: val_loss improved from 0.28576 to 0.28034, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1785 - val_loss: 0.2803\n",
      "Epoch 29/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1786Epoch 00028: val_loss improved from 0.28034 to 0.27656, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_32.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1785 - val_loss: 0.2766\n",
      "Epoch 30/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1785Epoch 00029: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1783 - val_loss: 0.2860\n",
      "Epoch 31/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1746Epoch 00030: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1745 - val_loss: 0.3047\n",
      "Epoch 32/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1694Epoch 00031: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1696 - val_loss: 0.2895\n",
      "Epoch 33/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1649Epoch 00032: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1651 - val_loss: 0.2934\n",
      "Epoch 34/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1652Epoch 00033: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1653 - val_loss: 0.2850\n",
      "Epoch 35/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1591Epoch 00034: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1591 - val_loss: 0.2935\n",
      "Epoch 36/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1582Epoch 00035: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1584 - val_loss: 0.2872\n",
      "Epoch 37/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1519Epoch 00036: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1523 - val_loss: 0.2988\n",
      "Epoch 38/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1530Epoch 00037: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1530 - val_loss: 0.3057\n",
      "Epoch 39/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1479Epoch 00038: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1477 - val_loss: 0.2957\n",
      "Epoch 40/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1476Epoch 00039: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1475 - val_loss: 0.3043\n",
      "Epoch 41/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1448Epoch 00040: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1448 - val_loss: 0.2987\n",
      "Epoch 42/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1453Epoch 00041: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1454 - val_loss: 0.2919\n",
      "Epoch 43/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1456Epoch 00042: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1454 - val_loss: 0.3023\n",
      "Epoch 44/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1438Epoch 00043: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1441 - val_loss: 0.2963\n",
      "Epoch 45/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1384Epoch 00044: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1384 - val_loss: 0.2952\n",
      "Epoch 46/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1386Epoch 00045: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1385 - val_loss: 0.2924\n",
      "Epoch 47/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1365Epoch 00046: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1365 - val_loss: 0.2982\n",
      "Epoch 48/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1346Epoch 00047: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1350 - val_loss: 0.3046\n",
      "Epoch 49/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1384Epoch 00048: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1386 - val_loss: 0.2888\n",
      "Epoch 50/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1299Epoch 00049: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1299 - val_loss: 0.3065\n",
      "Epoch 51/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1323Epoch 00050: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1325 - val_loss: 0.3127\n",
      "Epoch 52/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1334Epoch 00051: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1334 - val_loss: 0.2910\n",
      "Epoch 53/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1329Epoch 00052: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1331 - val_loss: 0.3028\n",
      "Epoch 54/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1300Epoch 00053: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1299 - val_loss: 0.3250\n",
      "Epoch 55/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1272Epoch 00054: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1273 - val_loss: 0.3027\n",
      "Epoch 56/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1247Epoch 00055: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1247 - val_loss: 0.2954\n",
      "Epoch 57/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1273Epoch 00056: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1274 - val_loss: 0.2941\n",
      "Epoch 58/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1274- EEpoch 00057: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1273 - val_loss: 0.2920\n",
      "Epoch 59/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1236Epoch 00058: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1237 - val_loss: 0.2924\n",
      "Epoch 60/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1241Epoch 00059: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1242 - val_loss: 0.3110\n",
      "Epoch 61/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1229Epoch 00060: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1229 - val_loss: 0.3002\n",
      "Epoch 62/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1231Epoch 00061: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1230 - val_loss: 0.2937\n",
      "Epoch 63/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1226Epoch 00062: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1227 - val_loss: 0.2997\n",
      "Epoch 64/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1195Epoch 00063: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1195 - val_loss: 0.3113\n",
      "Epoch 65/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1192Epoch 00064: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1192 - val_loss: 0.3006\n",
      "Epoch 66/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1220Epoch 00065: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1222 - val_loss: 0.2976\n",
      "Epoch 67/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1205Epoch 00066: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1203 - val_loss: 0.3234\n",
      "Epoch 68/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1201Epoch 00067: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1200 - val_loss: 0.2962\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1216Epoch 00068: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1217 - val_loss: 0.3013\n",
      "Epoch 70/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1177Epoch 00069: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1176 - val_loss: 0.2938\n",
      "Epoch 71/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1176Epoch 00070: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1176 - val_loss: 0.2907\n",
      "Epoch 72/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1175Epoch 00071: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1176 - val_loss: 0.2992\n",
      "Epoch 73/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1161Epoch 00072: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1163 - val_loss: 0.2837\n",
      "Epoch 74/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1175Epoch 00073: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1174 - val_loss: 0.3193\n",
      "Epoch 75/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1138Epoch 00074: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1139 - val_loss: 0.3001\n",
      "Epoch 76/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1153Epoch 00075: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1152 - val_loss: 0.2924\n",
      "Epoch 77/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1182Epoch 00076: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1181 - val_loss: 0.3015\n",
      "Epoch 78/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1151Epoch 00077: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1150 - val_loss: 0.2918\n",
      "Epoch 79/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1138Epoch 00078: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1137 - val_loss: 0.2862\n",
      "Epoch 80/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1141Epoch 00079: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1140 - val_loss: 0.3011\n",
      "Epoch 81/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1131Epoch 00080: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1131 - val_loss: 0.3010\n",
      "Epoch 82/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1160Epoch 00081: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1161 - val_loss: 0.2967\n",
      "Epoch 83/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1170Epoch 00082: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1171 - val_loss: 0.2918\n",
      "Epoch 84/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1139Epoch 00083: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1140 - val_loss: 0.2945\n",
      "Epoch 85/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1156Epoch 00084: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1157 - val_loss: 0.3022\n",
      "Epoch 86/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1106Epoch 00085: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1106 - val_loss: 0.3136\n",
      "Epoch 87/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1112Epoch 00086: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1116 - val_loss: 0.3004\n",
      "Epoch 88/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1104Epoch 00087: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1104 - val_loss: 0.2847\n",
      "Epoch 89/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1114Epoch 00088: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1114 - val_loss: 0.3064\n",
      "Epoch 90/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1130Epoch 00089: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1137 - val_loss: 0.3026\n",
      "Epoch 91/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1111Epoch 00090: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1110 - val_loss: 0.3066\n",
      "Epoch 92/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1124Epoch 00091: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1122 - val_loss: 0.3012\n",
      "Epoch 93/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1095Epoch 00092: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1094 - val_loss: 0.2981\n",
      "Epoch 94/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1142Epoch 00093: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1142 - val_loss: 0.3038\n",
      "Epoch 95/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1161Epoch 00094: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1160 - val_loss: 0.3276\n",
      "Epoch 96/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1187Epoch 00095: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1185 - val_loss: 0.3308\n",
      "Epoch 97/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1156Epoch 00096: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1156 - val_loss: 0.3053\n",
      "Epoch 98/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1165- ETA: 1s - loss:Epoch 00097: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1164 - val_loss: 0.3083\n",
      "Epoch 99/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1146Epoch 00098: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1144 - val_loss: 0.3127\n",
      "Epoch 100/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1108Epoch 00099: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1106 - val_loss: 0.3011\n",
      "Epoch 101/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1137Epoch 00100: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1137 - val_loss: 0.3212\n",
      "Epoch 102/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1122Epoch 00101: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1122 - val_loss: 0.3041\n",
      "Epoch 103/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1139Epoch 00102: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1139 - val_loss: 0.2948\n",
      "Epoch 104/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1137- ETA: 3sEpoch 00103: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1136 - val_loss: 0.2903\n",
      "Epoch 105/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1145Epoch 00104: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1147 - val_loss: 0.3064\n",
      "Epoch 106/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1134Epoch 00105: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1135 - val_loss: 0.3039\n",
      "Epoch 107/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1149Epoch 00106: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1148 - val_loss: 0.3186\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1189Epoch 00107: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1188 - val_loss: 0.3058\n",
      "Epoch 109/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1166Epoch 00108: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1165 - val_loss: 0.3168\n",
      "Epoch 110/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1183- ETA: 1s - loss:Epoch 00109: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1182 - val_loss: 0.3100\n",
      "Epoch 111/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1157Epoch 00110: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1157 - val_loss: 0.3038\n",
      "Epoch 112/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1169Epoch 00111: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1167 - val_loss: 0.3100\n",
      "Epoch 113/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1153Epoch 00112: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1151 - val_loss: 0.3244\n",
      "Epoch 114/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1190Epoch 00113: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1192 - val_loss: 0.3094\n",
      "Epoch 115/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1196Epoch 00114: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1197 - val_loss: 0.3119\n",
      "Epoch 116/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1156Epoch 00115: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1156 - val_loss: 0.2993\n",
      "Epoch 117/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1155Epoch 00116: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1156 - val_loss: 0.3148\n",
      "Epoch 118/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1184Epoch 00117: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1184 - val_loss: 0.3065\n",
      "Epoch 119/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1172Epoch 00118: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1172 - val_loss: 0.3171\n",
      "Epoch 120/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1199Epoch 00119: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1198 - val_loss: 0.3242\n",
      "Epoch 121/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1213Epoch 00120: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1211 - val_loss: 0.3223\n",
      "Epoch 122/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1202Epoch 00121: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1206 - val_loss: 0.3170\n",
      "Epoch 123/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1180Epoch 00122: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1179 - val_loss: 0.3151\n",
      "Epoch 124/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1195Epoch 00123: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1194 - val_loss: 0.3161\n",
      "Epoch 125/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1194Epoch 00124: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1195 - val_loss: 0.3097\n",
      "Epoch 126/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1178Epoch 00125: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1182 - val_loss: 0.3195\n",
      "Epoch 127/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1196Epoch 00126: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1195 - val_loss: 0.3075\n",
      "Epoch 128/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1207- ETEpoch 00127: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1207 - val_loss: 0.3115\n",
      "Epoch 129/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1176Epoch 00128: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1175 - val_loss: 0.3102\n",
      "Epoch 130/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1192Epoch 00129: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1191 - val_loss: 0.3216\n",
      "Epoch 131/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1195Epoch 00130: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1195 - val_loss: 0.3091\n",
      "Epoch 132/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1181Epoch 00131: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1181 - val_loss: 0.2945\n",
      "Epoch 133/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1221Epoch 00132: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1220 - val_loss: 0.3183\n",
      "Epoch 134/200\n",
      "15648/15706 [============================>.] - ETA: 0s - loss: 0.1268Epoch 00133: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1266 - val_loss: 0.3197\n",
      "Epoch 135/200\n",
      " 7776/15706 [=============>................] - ETA: 8s - loss: 0.1210"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    global_start_time = time.time()\n",
    "    startTime = datetime.now() \n",
    "    epochs  = 200\n",
    "    state_neurons_1 = 256\n",
    "    output = 1\n",
    "    seed = 7\n",
    "    batch_size = 32\n",
    "    scaler_type=\"standard\"\n",
    "    dropouts = [0.15, 0.25, 0.45]\n",
    "    seq_len = 48\n",
    "    \n",
    "    \n",
    "    batch_generator_obj = BatchGenerator(file=\"data_log/mumbai_6_log_pro.pkl\", time_steps=48, scaler_type=scaler_type, pca=False, pca_dim=8, normal=False)\n",
    "    X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp, y_norm, scaler = batch_generator_obj.return_data()\n",
    "    \n",
    "    X_train_pm, X_val_pm, X_train_ws, X_val_ws, X_train_rh, X_val_rh, X_train_bp, X_val_bp, X_train_vws, X_val_vws, X_train_sr, X_val_sr, X_train_wd, X_val_wd, X_train_temp, X_val_temp, y_train, y_val = train_test_split(X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp, y_norm, test_size=0.20, random_state=seed)\n",
    "    \n",
    "    print('> Data Loaded. Compiling...')\n",
    "    model = lstm.build_model([4, 7], [7, seq_len, state_neurons_1, state_neurons_1, output], dropouts)\n",
    "    \n",
    "    file_name = \"wt_GRU_Keras_linear_rmsprop_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    checkpoint = ModelCheckpoint(file_name+\".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    mod_hist = model.fit([X_train_pm, X_train_ws, X_train_rh, X_train_bp, X_train_vws, X_train_sr, X_train_wd, X_train_temp], [y_train], validation_data=([X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp], [y_val]), epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, shuffle=True)\n",
    "\n",
    "    print('Training duration (s) : ', time.time() - global_start_time)\n",
    "    print('Training duration (Hr) : ', datetime.now() - startTime)\n",
    "    \n",
    "    file = \"GRU_Keras_linear_rmsprop_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    print(\"Saving model : \"+file+\".h5\")\n",
    "    model.save(file+\".h5\")\n",
    "    print(\"Model saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #predict sequence\n",
    "    _y_norm = lstm.predict_point_by_point_aux(model, [X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp])\n",
    "    _y_train = lstm.predict_point_by_point_aux(model, [X_train_pm, X_train_ws, X_train_rh, X_train_bp, X_train_vws, X_train_sr, X_train_wd, X_train_temp])\n",
    "    _y_val = lstm.predict_point_by_point_aux(model, [X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    _y_norm = _y_norm.reshape(-1,1)\n",
    "    _y_train = _y_train.reshape(-1,1)\n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "    \n",
    "    y_norm = y_norm.reshape(-1,1)\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    \n",
    "    print(\"Predicted Output shape: \", np.shape(_y_norm))\n",
    "    print(\"Original Output shape:  \", np.shape(y_norm))\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y = scaler.inverse_transform(_y_norm)\n",
    "    _y_tr = scaler.inverse_transform(_y_train)\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "    for i in range(5):\n",
    "        print(_y_va[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y = scaler.inverse_transform(y_norm)\n",
    "    y_tr = scaler.inverse_transform(y_train)\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    for i in range(5):\n",
    "        print(y_va[i])    \n",
    "    \n",
    "    y = np.exp(y)\n",
    "    y_tr = np.exp(y_tr)\n",
    "    y_va = np.exp(y_va)\n",
    "\n",
    "    _y = np.exp(_y)\n",
    "    _y_va = np.exp(_y_va)\n",
    "    _y_tr = np.exp(_y_tr)\n",
    "    \n",
    "    #predicted\n",
    "    _Y = pd.Series(np.ravel(_y))\n",
    "    _Y_TR = pd.Series(np.ravel(_y_tr))\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "#     _Y_TE = pd.Series(np.ravel(_y_te))\n",
    "    \n",
    "    #original\n",
    "    Y =  pd.Series(np.ravel(y))\n",
    "    Y_TR = pd.Series(np.ravel(y_tr))\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "#     Y_TE = pd.Series(np.ravel(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    print('Total RMSE :', math.sqrt(mean_squared_error(_Y, Y)))\n",
    "    print('Training RMSE :', math.sqrt(mean_squared_error(_Y_TR, Y_TR)))\n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "#     print('Test RMSE :', math.sqrt(mean_squared_error(_Y_TE, Y_TE)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_VA[100:300], label='predicted')\n",
    "    plot_train, = plt.plot(Y_VA[100:300], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(mod_hist.history['loss'])\n",
    "plt.plot(mod_hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #recreate model \n",
    "    model = lstm.build_model([4, 7], [7, seq_len, state_neurons_1, state_neurons_1, output], dropouts, pre_train=file_name+\".h5\")\n",
    "    print(\"Created model and loaded weights from \"+file_name+\".h5\")\n",
    "    \n",
    "    # estimate accuracy on whole dataset using loaded weights\n",
    "    _y_val = lstm.predict_point_by_point_aux(model, [X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp])\n",
    "    \n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "    \n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "    for i in range(5):\n",
    "        print(_y_va[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    for i in range(5):\n",
    "        print(y_va[i])    \n",
    "    \n",
    "    \n",
    "    y_va = np.exp(y_va)\n",
    "    _y_va = np.exp(_y_va)\n",
    "    \n",
    "    #predicted\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "    \n",
    "    #original\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "    \n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_VA[100:300], label='predicted')\n",
    "    plot_train, = plt.plot(Y_VA[100:300], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(x):\n",
    "    return float(1.0 * x/(x+10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
