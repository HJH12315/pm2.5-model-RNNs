{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-bb68f60c65b9>\", line 12, in <module>\n",
      "    from sklearn.decomposition import PCA\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\sklearn\\decomposition\\__init__.py\", line 10, in <module>\n",
      "    from .kernel_pca import KernelPCA\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\sklearn\\decomposition\\kernel_pca.py\", line 14, in <module>\n",
      "    from ..preprocessing import KernelCenterer\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\sklearn\\preprocessing\\__init__.py\", line 8, in <module>\n",
      "    from .data import Binarizer\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\sklearn\\preprocessing\\data.py\", line 18, in <module>\n",
      "    from scipy import stats\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\scipy\\stats\\__init__.py\", line 348, in <module>\n",
      "    from .stats import *\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\scipy\\stats\\stats.py\", line 177, in <module>\n",
      "    from . import distributions\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\scipy\\stats\\distributions.py\", line 13, in <module>\n",
      "    from . import _continuous_distns\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\scipy\\stats\\_continuous_distns.py\", line 2488, in <module>\n",
      "    halflogistic = halflogistic_gen(a=0.0, name='halflogistic')\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\", line 1547, in __init__\n",
      "    self._construct_doc(docdict, dct.get(self.name))\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\", line 735, in _construct_doc\n",
      "    self.__doc__ = doccer.docformat(self.__doc__, tempdict)\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\scipy\\misc\\doccer.py\", line 56, in docformat\n",
      "    indent = ' ' * icount\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\inspect.py\", line 1459, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\inspect.py\", line 1417, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\inspect.py\", line 677, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\inspect.py\", line 720, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\inspect.py\", line 689, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\inspect.py\", line 674, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\Razer\\AppData\\Local\\conda\\conda\\envs\\keras-tf-gpu-dev\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "import lstm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, Panel\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import logistic\n",
    "%matplotlib inline\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "print (\"Current size:\", fig_size)\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 24\n",
    "fig_size[1] = 18\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "keras.backend.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, file, time_steps, scaler_type):\n",
    "        print (\"Loading data ...\")\n",
    "        \n",
    "#         print('Processing CSV :', file)\n",
    "#         data_temp = fill_gaps(data, \"SR\", 0.05)\n",
    "#         data_temp = fill_gaps(data_temp, \"WD\", 2.0)\n",
    "#         data_temp = fill_gaps(data_temp, \"WS\", 0.005)\n",
    "#         data_temp = fill_gaps(data_temp, \"VWS\", 0.005)\n",
    "#         data_temp = fill_gaps(data_temp, \"BP\", 0.1)\n",
    "#         data_temp = fill_gaps(data_temp, \"TEMP\", 0.05)\n",
    "#         data_temp = fill_gaps(data_temp, \"PM2.5\", 0.05)\n",
    "#         data_temp = fill_gaps(data_temp, \"RH\", 0.05)\n",
    "        \n",
    "        data_loaded = pd.read_pickle(file)\n",
    "        data_loaded.isnull().sum()\n",
    "        data_loaded_np = data_loaded[[\"PM2.5\",\"WS\",\"RH\",\"BP\",\"VWS\",\"SR\",\"WD\",\"TEMP\"]].as_matrix()\n",
    "        \n",
    "        self.time_steps = time_steps\n",
    "        self.scaler_type = scaler_type\n",
    "        self.X_norm_pm, self.y_norm_pm, self.scaler_pm, self.min_max_scaler_pm = self.generate_batch_data(data_loaded_np[0:,0], time_steps=self.time_steps, name=\"pm25\")\n",
    "        self.X_norm_ws, self.y_norm_ws, self.scaler_ws, self.min_max_scaler_ws = self.generate_batch_data(data_loaded_np[0:,1], time_steps=self.time_steps, name=\"ws\")\n",
    "        self.X_norm_rh, self.y_norm_rh, self.scaler_rh, self.min_max_scaler_rh = self.generate_batch_data(data_loaded_np[0:,2], time_steps=self.time_steps, name=\"rh\")\n",
    "        self.X_norm_bp, self.y_norm_bp, self.scaler_bp, self.min_max_scaler_bp = self.generate_batch_data(data_loaded_np[0:,4], time_steps=self.time_steps, name=\"bp\")\n",
    "        self.X_norm_vws, self.y_norm_vws, self.scaler_vws, self.min_max_scaler_vws = self.generate_batch_data(data_loaded_np[0:,3], time_steps=self.time_steps, name=\"vws\")\n",
    "        self.X_norm_sr, self.y_norm_sr, self.scaler_sr, self.min_max_scaler_sr = self.generate_batch_data(data_loaded_np[0:,5], time_steps=self.time_steps, name=\"sr\")\n",
    "        self.X_norm_wd, self.y_norm_wd, self.scaler_wd, self.min_max_scaler_wd = self.generate_batch_data(data_loaded_np[0:,6], time_steps=self.time_steps, name=\"wd\")\n",
    "        self.X_norm_temp, self.y_norm_temp, self.scaler_temp, self.min_max_scaler_temp = self.generate_batch_data(data_loaded_np[0:,7], time_steps=self.time_steps, name=\"temp\")\n",
    "\n",
    "        filename = \"np_\"+self.scaler_type+\"_process_comp_\"+str(self.time_steps)+\".npz\"\n",
    "        if os.path.isfile(\"data_log/\"+filename):\n",
    "            print (\"Found existing file :\",filename)\n",
    "            print (\"Loading ...\")\n",
    "            npzfile = np.load(\"data_log/\"+filename)\n",
    "            self.X = npzfile['arr_0']\n",
    "            self.Y = npzfile['arr_1']\n",
    "            print (\"Complete.\")\n",
    "        else:\n",
    "            self.X = np.array(np.zeros([1, 8]))\n",
    "            for i in range(len(self.X_norm_pm)):\n",
    "                temp = np.column_stack((self.X_norm_pm[i],self.X_norm_ws[i],self.X_norm_rh[i],self.X_norm_bp[i],self.X_norm_vws[i],self.X_norm_sr[i],self.X_norm_wd[i],self.X_norm_temp[i]))\n",
    "                self.X = np.append(self.X, temp, axis=0)\n",
    "\n",
    "            self.X = self.X[1:].reshape(len(self.X_norm_pm),48,8)\n",
    "            self.Y = self.y_norm_pm\n",
    "            print (\"Input shape :\",np.shape(self.X))\n",
    "            print (\"Output shape :\",np.shape(self.Y))\n",
    "            print (\"Saving file ...\")\n",
    "            np.savez(\"data_log/\"+filename, self.X, self.Y)\n",
    "            print (\"Saved file to :\", \"data_log/\"+filename)\n",
    "            print (\"Complete.\")\n",
    "        \n",
    "    def return_data(self):\n",
    "        return self.X, self.Y, self.scaler_pm, self.min_max_scaler_pm\n",
    "    \n",
    "    def fill_gaps(self, data, col, sigma):\n",
    "        temp = data.copy()\n",
    "        temp[\"FROM\"][0] = \"00:00:00\"\n",
    "\n",
    "        mu, sigma = 0, sigma \n",
    "        for k in range(len(temp)):\n",
    "            try:\n",
    "                if (str(temp[col][k]) == str(np.nan)):\n",
    "                    rolling_sum = 0\n",
    "                    noise = np.random.normal(mu, sigma, 1)[0]\n",
    "                    rolling_sum = rolling_sum + float(temp[col][k-24]) + float(temp[col][k-48]) + float(temp[col][k-72]) + float(temp[col][k-96]) + float(temp[col][k-120]) + float(temp[col][k-144]) + float(temp[col][k-168])\n",
    "                    temp[col][k] = round(1.0*rolling_sum/7 + noise,2)\n",
    "            except (IndexError, ValueError):\n",
    "                print (\"Break at index :\", k)\n",
    "                break\n",
    "        return temp\n",
    "\n",
    "    def shift(self, arr, num, fill_value=np.nan):\n",
    "        result = np.empty_like(arr)\n",
    "        if num > 0:\n",
    "            result[:num] = fill_value\n",
    "            result[num:] = arr[:-num]\n",
    "        elif num < 0:\n",
    "            result[num:] = fill_value\n",
    "            result[:num] = arr[-num:]\n",
    "        else:\n",
    "            result = arr\n",
    "        return result\n",
    "\n",
    "    def generate_batch_data(self, raw_data, time_steps, name):\n",
    "        series = pd.Series(raw_data, dtype=np.float32)\n",
    "        # prepare data for standardization\n",
    "        values = series.values\n",
    "        values = values.reshape((len(values), 1))\n",
    "\n",
    "        # train the standardization\n",
    "        if self.scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        if self.scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        if self.scaler_type == 'min_max':\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        if self.scaler_type == 'robust_min_max':\n",
    "            scaler = RobustScaler()\n",
    "        if self.scaler_type == 'standard_min_max':\n",
    "            scaler = StandardScaler()\n",
    "            \n",
    "        min_max_scaler = None    \n",
    "        scaler = scaler.fit(values)\n",
    "        print('feature ------------ ', name.upper())\n",
    "        \n",
    "        if self.scaler_type == 'standard':\n",
    "            print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, math.sqrt(scaler.var_)))\n",
    "            values[values > 450] = 3*scaler.mean_\n",
    "            print('Data normalized... Replaced the outliers with 3 times the mean value')\n",
    "        if self.scaler_type == 'robust':\n",
    "            print('Data normalized... Using Robust Scaling')\n",
    "        if self.scaler_type == 'min_max':\n",
    "            print('Data normalized... Using Min-Max Scaling')\n",
    "       \n",
    "        normalized = scaler.transform(values)\n",
    "        \n",
    "        # min_max scaling\n",
    "        if self.scaler_type == 'robust_min_max' or self.scaler_type == 'standard_min_max':\n",
    "            min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            min_max_scaler = min_max_scaler.fit(normalized)\n",
    "            normalized = min_max_scaler.transform(normalized)\n",
    "            if self.scaler_type == 'robust_min_max':\n",
    "                print('Data normalized... Using Robust_Min-Max Scaling')\n",
    "            if self.scaler_type == 'standard_min_max':\n",
    "                print('Data normalized... Using Standard_Min-Max Scaling')\n",
    "            \n",
    "        # batch formation\n",
    "        data = normalized\n",
    "        print('Max: %f, Min: %f' % (np.amax(data), np.amin(data)))\n",
    "        x = data[:(len(data)-(len(data) % time_steps))]\n",
    "        y = self.shift(data,-(time_steps)).astype(np.float32)\n",
    "\n",
    "        x_batches = np.array([])\n",
    "        y_batches = np.array([])\n",
    "\n",
    "        # check if file exists\n",
    "        seq_file_name = \"np_\"+self.scaler_type+\"_processed_\"+name+\"_\"+str(time_steps)+\".npz\"\n",
    "        if os.path.isfile(\"data_log/\"+seq_file_name):\n",
    "            npzfile = np.load(\"data_log/\"+seq_file_name)\n",
    "            x_batches = npzfile['arr_0']\n",
    "            y_batches = npzfile['arr_1']\n",
    "            return x_batches, y_batches, scaler, min_max_scaler\n",
    "        else: \n",
    "            for i in range(len(y)):\n",
    "                try:\n",
    "                    x_batches = np.append(x_batches, x[i:i+time_steps].reshape(-1,12,1))\n",
    "                    y_batches = np.append(y_batches, y[i].reshape(-1))\n",
    "                except ValueError:\n",
    "                    break\n",
    "            x_batches = x_batches.reshape(-1, time_steps, 1)\n",
    "            y_batches = y_batches.reshape(-1)\n",
    "            np.savez(\"data_log/\"+seq_file_name, x_batches, y_batches)\n",
    "            return x_batches, y_batches, scaler, min_max_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    global_start_time = time.time()\n",
    "    startTime = datetime.now() \n",
    "    epochs  = 150\n",
    "    state_neurons_1 = 256\n",
    "    seed = 7\n",
    "    batch_size = 32\n",
    "    scaler_type=\"min_max\"\n",
    "    dropouts = [0.05, 0.14, 0.35]\n",
    "    seq_len = 48\n",
    "    \n",
    "    batch_generator_obj = BatchGenerator(file=\"data_log/mumbai_6_log_pro.pkl\", time_steps=seq_len, scaler_type=scaler_type)\n",
    "    X_norm, y_norm, scaler, min_max_scaler = batch_generator_obj.return_data()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_norm, y_norm, test_size=0.20, random_state=seed)\n",
    "#     X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=seed)\n",
    "    \n",
    "    print('> Data Loaded. Compiling...')\n",
    "    model = lstm.build_model([4, 8], [8, seq_len, state_neurons_1, 1, state_neurons_1], dropouts)\n",
    "    \n",
    "    file_name = \"wt_LSTM_Keras_multi_adam_autoen_imp2_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    checkpoint = ModelCheckpoint(file_name+\".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    mod_hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, shuffle=True)\n",
    "\n",
    "    print('Training duration (s) : ', time.time() - global_start_time)\n",
    "    print('Training duration (Hr) : ', datetime.now() - startTime)\n",
    "    \n",
    "    file = \"LSTM_Keras_multi_adam_autoen_imp2_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    print(\"Saving model : \"+file+\".h5\")\n",
    "    model.save(file+\".h5\")\n",
    "    print(\"Model saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #predict sequence\n",
    "    _y_norm = lstm.predict_point_by_point(model, X_norm)\n",
    "    _y_train = lstm.predict_point_by_point(model, X_train)\n",
    "    _y_val = lstm.predict_point_by_point(model, X_val)\n",
    "#     _y_test = lstm.predict_point_by_point(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    _y_norm = _y_norm.reshape(-1,1)\n",
    "    _y_train = _y_train.reshape(-1,1)\n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "#     _y_test = _y_test.reshape(-1,1)\n",
    "    \n",
    "    y_norm = y_norm.reshape(-1,1)\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_val = y_val.reshape(-1,1)\n",
    "#     y_test = y_test.reshape(-1,1)\n",
    "    \n",
    "    print(\"Predicted Output shape: \", np.shape(_y_norm))\n",
    "    print(\"Original Output shape:  \", np.shape(y_norm))\n",
    "    \n",
    "    if not (min_max_scaler is None):\n",
    "        print(\"Using Min_Max Inversion ...\")\n",
    "        _y_norm = min_max_scaler.inverse_transform(_y_norm)\n",
    "        _y_train = min_max_scaler.inverse_transform(_y_train)\n",
    "        _y_val = min_max_scaler.inverse_transform(_y_val)\n",
    "#         _y_test = min_max_scaler.inverse_transform(_y_test)\n",
    "        \n",
    "        y_norm = min_max_scaler.inverse_transform(y_norm)\n",
    "        y_train = min_max_scaler.inverse_transform(y_train)\n",
    "        y_val = min_max_scaler.inverse_transform(y_val)\n",
    "#         y_test = min_max_scaler.inverse_transform(y_test)\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y = scaler.inverse_transform(_y_norm)\n",
    "    _y_tr = scaler.inverse_transform(_y_train)\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "#     _y_te = scaler.inverse_transform(_y_test)\n",
    "\n",
    "    _y_tr = np.exp(_y_tr)\n",
    "    _y_va = np.exp(_y_va)\n",
    "    _y = np.exp(_y)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(_y[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y = scaler.inverse_transform(y_norm)\n",
    "    y_tr = scaler.inverse_transform(y_train)\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    \n",
    "    y_tr = np.exp(y_tr)\n",
    "    y_va = np.exp(y_va)\n",
    "    y = np.exp(y)\n",
    "#     y_te = scaler.inverse_transform(y_test)\n",
    "    for i in range(5):\n",
    "        print(y[i])    \n",
    "    \n",
    "    #predicted\n",
    "    _Y = pd.Series(np.ravel(_y))\n",
    "    _Y_TR = pd.Series(np.ravel(_y_tr))\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "#     _Y_TE = pd.Series(np.ravel(_y_te))\n",
    "    \n",
    "    #original\n",
    "    Y =  pd.Series(np.ravel(y))\n",
    "    Y_TR = pd.Series(np.ravel(y_tr))\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "#     Y_TE = pd.Series(np.ravel(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    print('Training RMSE :', math.sqrt(mean_squared_error(_Y_TR, Y_TR)))\n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "#     print('Test RMSE :', math.sqrt(mean_squared_error(_Y_TE, Y_TE)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_VA[100:200], label='predicted')\n",
    "    plot_train, = plt.plot(Y_VA[100:200], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(mod_hist.history['loss'])\n",
    "plt.plot(mod_hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
