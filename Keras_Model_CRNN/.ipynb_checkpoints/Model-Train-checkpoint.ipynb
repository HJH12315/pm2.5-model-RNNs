{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size: [6.0, 4.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lstm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame, Panel\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import logistic\n",
    "%matplotlib inline\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Prints: [8.0, 6.0]\n",
    "print (\"Current size:\", fig_size)\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 24\n",
    "fig_size[1] = 18\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "keras.backend.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    \n",
    "    def __init__(self, file, time_steps=48, scaler_type='standard', pca=False, pca_dim=8, normal=False):\n",
    "        self.file = file\n",
    "        self.time_steps = time_steps\n",
    "        self.scaler_type = scaler_type\n",
    "        self.pca = pca\n",
    "        self.pca_dim = pca_dim\n",
    "        self.normal = normal\n",
    "        \n",
    "        print (\"Loading data ...\")\n",
    "        data_loaded = pd.read_pickle(self.file)\n",
    "        data_loaded.isnull().sum()\n",
    "        \n",
    "        # load data\n",
    "        data_loaded_np = data_loaded[[\"PM2.5\",\"WS\",\"RH\",\"BP\",\"VWS\",\"SR\",\"WD\",\"TEMP\"]].as_matrix()\n",
    "       \n",
    "        # PCA\n",
    "        if self.pca == True:\n",
    "            print(\"PCA transform\")\n",
    "            pca = PCA(n_components=self.pca_dim, svd_solver='full')\n",
    "            pca = pca.fit(data_loaded_np)\n",
    "            data_loaded_np = pca.transform(data_loaded_np)\n",
    "        \n",
    "        #Row Normalization    \n",
    "        if self.normal == True:\n",
    "            print(\"Normalize transform\")\n",
    "            self.norm_scaler = Normalizer().fit(data_loaded_np)\n",
    "            data_loaded_np = self.norm_scaler.transform(data_loaded_np)\n",
    "\n",
    "        self.X_norm_pm, self.y_norm_pm, self.scaler_pm = self.generate_batch_data(data_loaded_np[0:,0], time_steps=self.time_steps, name=\"pm25\")\n",
    "        self.X_norm_ws, self.y_norm_ws, self.scaler_ws = self.generate_batch_data(data_loaded_np[0:,1], time_steps=self.time_steps, name=\"ws\")\n",
    "        self.X_norm_rh, self.y_norm_rh, self.scaler_rh = self.generate_batch_data(data_loaded_np[0:,2], time_steps=self.time_steps, name=\"rh\")\n",
    "        self.X_norm_bp, self.y_norm_bp, self.scaler_bp = self.generate_batch_data(data_loaded_np[0:,3], time_steps=self.time_steps, name=\"bp\")\n",
    "        self.X_norm_vws, self.y_norm_vws, self.scaler_vws = self.generate_batch_data(data_loaded_np[0:,4], time_steps=self.time_steps, name=\"vws\")\n",
    "        self.X_norm_sr, self.y_norm_sr, self.scaler_sr = self.generate_batch_data(data_loaded_np[0:,5], time_steps=self.time_steps, name=\"sr\")\n",
    "        self.X_norm_wd, self.y_norm_wd, self.scaler_wd = self.generate_batch_data(data_loaded_np[0:,6], time_steps=self.time_steps, name=\"wd\")\n",
    "        self.X_norm_temp, self.y_norm_temp, self.scaler_temp = self.generate_batch_data(data_loaded_np[0:,7], time_steps=self.time_steps, name=\"temp\")\n",
    "\n",
    "        if not (self.scaler_type is None):\n",
    "            filename = \"np_\"+self.scaler_type+\"_process_comp_\"+str(self.time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "        else:\n",
    "            filename = \"np_process_comp_\"+str(self.time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "\n",
    "        if os.path.isfile(\"data_log/\"+filename):\n",
    "            print (\"Found existing file :\",\"data_log/\"+filename)\n",
    "            print (\"Loading ...\")\n",
    "            npzfile = np.load(\"data_log/\"+filename)\n",
    "            self.X_norm_pm = npzfile['arr_0']\n",
    "            self.X_norm_ws = npzfile['arr_1']\n",
    "            self.X_norm_rh = npzfile['arr_2']\n",
    "            self.X_norm_bp = npzfile['arr_3']\n",
    "            self.X_norm_vws = npzfile['arr_4']\n",
    "            self.X_norm_sr = npzfile['arr_5']\n",
    "            self.X_norm_wd = npzfile['arr_6']\n",
    "            self.X_norm_temp = npzfile['arr_7']\n",
    "            self.Y = npzfile['arr_8']\n",
    "            print (\"Complete.\")\n",
    "        else:\n",
    "            self.Y = self.y_norm_pm \n",
    "            print (\"Input shape pm25:\",np.shape(self.X_norm_pm))\n",
    "            print (\"Input shape ws:\",np.shape(self.X_norm_ws))\n",
    "            print (\"Input shape rh:\",np.shape(self.X_norm_rh))\n",
    "            print (\"Input shape bp:\",np.shape(self.X_norm_bp))\n",
    "            print (\"Input shape vws:\",np.shape(self.X_norm_vws))\n",
    "            print (\"Input shape sr:\",np.shape(self.X_norm_sr))\n",
    "            print (\"Input shape wd:\",np.shape(self.X_norm_wd))\n",
    "            print (\"Input shape temp:\",np.shape(self.X_norm_temp))\n",
    "            print (\"Output shape :\",np.shape(self.Y))\n",
    "            print (\"Saving file ...\")\n",
    "            np.savez(\"data_log/\"+filename, self.X_norm_pm, self.X_norm_ws, self.X_norm_rh, self.X_norm_bp, self.X_norm_vws, self.X_norm_sr, self.X_norm_wd, self.X_norm_temp, self.Y)\n",
    "            print (\"Saved file to :\", filename)\n",
    "            print (\"Complete.\")\n",
    "        \n",
    "    def return_data(self):\n",
    "        return self.X_norm_pm, self.X_norm_ws, self.X_norm_rh, self.X_norm_bp, self.X_norm_vws, self.X_norm_sr, self.X_norm_wd, self.X_norm_temp, self.Y, self.scaler_pm\n",
    "\n",
    "    def shift(self, arr, num, fill_value=np.nan):\n",
    "        result = np.empty_like(arr)\n",
    "        if num > 0:\n",
    "            result[:num] = fill_value\n",
    "            result[num:] = arr[:-num]\n",
    "        elif num < 0:\n",
    "            result[num:] = fill_value\n",
    "            result[:num] = arr[-num:]\n",
    "        else:\n",
    "            result = arr\n",
    "        return result\n",
    "\n",
    "    def generate_batch_data(self, raw_data, time_steps, name):\n",
    "        series = pd.Series(raw_data, dtype=np.float32)\n",
    "        values = series.values\n",
    "        values = values.reshape((len(values), 1))\n",
    "        print('feature ------------ ', name.upper())\n",
    "        \n",
    "        if self.scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        if self.scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        if self.scaler_type == 'min_max':\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            \n",
    "        scaler = scaler.fit(values)\n",
    "        normalized = scaler.transform(values)\n",
    "        \n",
    "        #data = values\n",
    "        data = normalized\n",
    "        print('Max: %f, Min: %f' % (np.amax(data), np.amin(data)))\n",
    "        x = data[:(len(data)-(len(data) % time_steps))]\n",
    "        y = self.shift(data,-(time_steps)).astype(np.float32)\n",
    "\n",
    "        x_batches = np.array([])\n",
    "        y_batches = np.array([])\n",
    "\n",
    "        # check if file exists\n",
    "        if (self.scaler_type is None):\n",
    "            seq_file_name = \"np_processed_\"+name+\"_\"+str(time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"\n",
    "        else:\n",
    "            seq_file_name = \"np_\"+self.scaler_type+\"_processed_\"+name+\"_\"+str(time_steps)+\"_\"+str(self.pca)+\"_\"+str(self.normal)+\".npz\"          \n",
    "\n",
    "        if os.path.isfile(\"data_log/\"+seq_file_name):\n",
    "            npzfile = np.load(\"data_log/\"+seq_file_name)\n",
    "            x_batches = npzfile['arr_0']\n",
    "            y_batches = npzfile['arr_1']\n",
    "            return x_batches, y_batches, scaler\n",
    "        else: \n",
    "            for i in range(len(y)):\n",
    "                try:\n",
    "                    x_batches = np.append(x_batches, x[i:i+(time_steps)].reshape(-1,12,1))\n",
    "                    y_batches = np.append(y_batches, y[i].reshape(-1))\n",
    "                except ValueError:\n",
    "                    break\n",
    "                    \n",
    "            x_batches = x_batches.reshape(-1, time_steps, 1)\n",
    "            y_batches = y_batches.reshape(-1)\n",
    "            np.savez(\"data_log/\"+seq_file_name, x_batches, y_batches)\n",
    "            return x_batches, y_batches, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "feature ------------  PM25\n",
      "Max: 3.284291, Min: -10.752898\n",
      "feature ------------  WS\n",
      "Max: 3.415116, Min: -2.449860\n",
      "feature ------------  RH\n",
      "Max: 1.363076, Min: -4.063657\n",
      "feature ------------  BP\n",
      "Max: 5.360242, Min: -7.499539\n",
      "feature ------------  VWS\n",
      "Max: 9.401879, Min: -11.963299\n",
      "feature ------------  SR\n",
      "Max: 1.637784, Min: -2.654759\n",
      "feature ------------  WD\n",
      "Max: 1.076242, Min: -6.484889\n",
      "feature ------------  TEMP\n",
      "Max: 3.006956, Min: -3.982146\n",
      "Found existing file : data_log/np_standard_process_comp_48_False_False.npz\n",
      "Loading ...\n",
      "Complete.\n",
      "> Data Loaded. Compiling...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 48, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 41, 128)      1152        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 41, 128)      1152        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 41, 128)      1152        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 41, 128)      1152        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 41, 128)      1152        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 41, 128)      1152        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 41, 128)      1152        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 41, 128)      1152        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 41, 128)      512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 41, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 41, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 10, 128)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 41, 128)      512         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 41, 128)      512         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 41, 128)      512         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 41, 128)      512         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 10, 128)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 10, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 10, 128)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10, 128)      512         max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 10, 128)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 10, 128)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 10, 128)      0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 10, 128)      0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 1024)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)          (None, 10, 256)      984576      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 10, 256)      0           cu_dnngru_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)          (None, 10, 256)      394752      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10, 256)      0           cu_dnngru_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 10, 256)      65792       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 10, 256)      1024        time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 10, 1)        257         batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 10, 1)        4           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            11          flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,459,728\n",
      "Trainable params: 1,457,166\n",
      "Non-trainable params: 2,562\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "> Compilation Time :  0.03410649299621582\n",
      "Train on 15706 samples, validate on 3927 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.5235Epoch 00000: val_loss improved from inf to 0.47647, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.5232 - val_loss: 0.4765\n",
      "Epoch 2/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.4407Epoch 00001: val_loss improved from 0.47647 to 0.45039, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.4403 - val_loss: 0.4504\n",
      "Epoch 3/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.4048Epoch 00002: val_loss improved from 0.45039 to 0.41585, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.4052 - val_loss: 0.4159\n",
      "Epoch 4/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.3754Epoch 00003: val_loss improved from 0.41585 to 0.39101, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3751 - val_loss: 0.3910\n",
      "Epoch 5/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.3479Epoch 00004: val_loss improved from 0.39101 to 0.38554, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3484 - val_loss: 0.3855\n",
      "Epoch 6/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.3272Epoch 00005: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3268 - val_loss: 0.4014\n",
      "Epoch 7/200\n",
      "15552/15706 [============================>.] - ETA: 0s - loss: 0.3049Epoch 00006: val_loss improved from 0.38554 to 0.36445, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.3046 - val_loss: 0.3644\n",
      "Epoch 8/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.2877Epoch 00007: val_loss improved from 0.36445 to 0.36214, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2876 - val_loss: 0.3621\n",
      "Epoch 9/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.2738Epoch 00008: val_loss improved from 0.36214 to 0.34652, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2739 - val_loss: 0.3465\n",
      "Epoch 10/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.2609Epoch 00009: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2610 - val_loss: 0.3632\n",
      "Epoch 11/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.2448Epoch 00010: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2450 - val_loss: 0.3531\n",
      "Epoch 12/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.2317Epoch 00011: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2318 - val_loss: 0.3560\n",
      "Epoch 13/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.2206Epoch 00012: val_loss improved from 0.34652 to 0.34438, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2207 - val_loss: 0.3444\n",
      "Epoch 14/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.2117Epoch 00013: val_loss improved from 0.34438 to 0.33471, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.2136 - val_loss: 0.3347\n",
      "Epoch 15/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1975Epoch 00014: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1975 - val_loss: 0.3357\n",
      "Epoch 16/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1932Epoch 00015: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1933 - val_loss: 0.3411\n",
      "Epoch 17/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1837Epoch 00016: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1834 - val_loss: 0.3515\n",
      "Epoch 18/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1751Epoch 00017: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1755 - val_loss: 0.3559\n",
      "Epoch 19/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1730Epoch 00018: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1730 - val_loss: 0.3368\n",
      "Epoch 20/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1613Epoch 00019: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1613 - val_loss: 0.3410\n",
      "Epoch 21/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1555Epoch 00020: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1555 - val_loss: 0.3480\n",
      "Epoch 22/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1497Epoch 00021: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1497 - val_loss: 0.3349\n",
      "Epoch 23/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1458Epoch 00022: val_loss improved from 0.33471 to 0.32900, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1459 - val_loss: 0.3290\n",
      "Epoch 24/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1399Epoch 00023: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1399 - val_loss: 0.3383\n",
      "Epoch 25/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1344Epoch 00024: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1344 - val_loss: 0.3392\n",
      "Epoch 26/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1319Epoch 00025: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1320 - val_loss: 0.3369\n",
      "Epoch 27/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1254Epoch 00026: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1253 - val_loss: 0.3332\n",
      "Epoch 28/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1288Epoch 00027: val_loss improved from 0.32900 to 0.32197, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1286 - val_loss: 0.3220\n",
      "Epoch 29/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1192Epoch 00028: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1190 - val_loss: 0.3275\n",
      "Epoch 30/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1150Epoch 00029: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1151 - val_loss: 0.3355\n",
      "Epoch 31/200\n",
      "15616/15706 [============================>.] - ETA: 0s - loss: 0.1159Epoch 00030: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1157 - val_loss: 0.3298\n",
      "Epoch 32/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1095Epoch 00031: val_loss did not improve\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1094 - val_loss: 0.3408\n",
      "Epoch 33/200\n",
      "15680/15706 [============================>.] - ETA: 0s - loss: 0.1090Epoch 00032: val_loss improved from 0.32197 to 0.31878, saving model to wt_GRU_Keras_linear_rmsprop_standard_dp(0.15_0.25_0.45)_200_48_256_256_64.h5\n",
      "15706/15706 [==============================] - ETA: 0s - loss: 0.1091 - val_loss: 0.3188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200\n",
      "11776/15706 [=====================>........] - ETA: 2s - loss: 0.1009"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    global_start_time = time.time()\n",
    "    startTime = datetime.now() \n",
    "    epochs  = 200\n",
    "    state_neurons_1 = 256\n",
    "    output = 1\n",
    "    seed = 7\n",
    "    batch_size = 32\n",
    "    scaler_type=\"standard\"\n",
    "    dropouts = [0.15, 0.25, 0.45]\n",
    "    seq_len = 48\n",
    "    \n",
    "    \n",
    "    batch_generator_obj = BatchGenerator(file=\"data_log/mumbai_6_log_pro.pkl\", time_steps=48, scaler_type=scaler_type, pca=False, pca_dim=8, normal=False)\n",
    "    X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp, y_norm, scaler = batch_generator_obj.return_data()\n",
    "    \n",
    "    X_train_pm, X_val_pm, X_train_ws, X_val_ws, X_train_rh, X_val_rh, X_train_bp, X_val_bp, X_train_vws, X_val_vws, X_train_sr, X_val_sr, X_train_wd, X_val_wd, X_train_temp, X_val_temp, y_train, y_val = train_test_split(X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp, y_norm, test_size=0.20, random_state=seed)\n",
    "    \n",
    "    print('> Data Loaded. Compiling...')\n",
    "    model = lstm.build_model([4, 7], [7, seq_len, state_neurons_1, state_neurons_1, output], dropouts)\n",
    "    \n",
    "    file_name = \"wt_GRU_Keras_linear_rmsprop_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    checkpoint = ModelCheckpoint(file_name+\".h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    mod_hist = model.fit([X_train_pm, X_train_ws, X_train_rh, X_train_bp, X_train_vws, X_train_sr, X_train_wd, X_train_temp], [y_train], validation_data=([X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp], [y_val]), epochs=epochs, batch_size=batch_size, callbacks=callbacks_list, shuffle=True)\n",
    "\n",
    "    print('Training duration (s) : ', time.time() - global_start_time)\n",
    "    print('Training duration (Hr) : ', datetime.now() - startTime)\n",
    "    \n",
    "    file = \"GRU_Keras_linear_rmsprop_\"+scaler_type+\"_dp(\"+\"_\".join(str(x) for x in dropouts)+\")_\"+str(epochs)+\"_\"+str(seq_len)+\"_\"+str(state_neurons_1)+\"_\"+str(state_neurons_1)+\"_\"+str(batch_size)\n",
    "    print(\"Saving model : \"+file+\".h5\")\n",
    "    model.save(file+\".h5\")\n",
    "    print(\"Model saved...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #predict sequence\n",
    "    _y_norm = lstm.predict_point_by_point_aux(model, [X_norm_pm, X_norm_ws, X_norm_rh, X_norm_bp, X_norm_vws, X_norm_sr, X_norm_wd, X_norm_temp])\n",
    "    _y_train = lstm.predict_point_by_point_aux(model, [X_train_pm, X_train_ws, X_train_rh, X_train_bp, X_train_vws, X_train_sr, X_train_wd, X_train_temp])\n",
    "    _y_val = lstm.predict_point_by_point_aux(model, [X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    _y_norm = _y_norm.reshape(-1,1)\n",
    "    _y_train = _y_train.reshape(-1,1)\n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "    \n",
    "    y_norm = y_norm.reshape(-1,1)\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    \n",
    "    print(\"Predicted Output shape: \", np.shape(_y_norm))\n",
    "    print(\"Original Output shape:  \", np.shape(y_norm))\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y = scaler.inverse_transform(_y_norm)\n",
    "    _y_tr = scaler.inverse_transform(_y_train)\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "    for i in range(5):\n",
    "        print(_y_va[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y = scaler.inverse_transform(y_norm)\n",
    "    y_tr = scaler.inverse_transform(y_train)\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    for i in range(5):\n",
    "        print(y_va[i])    \n",
    "    \n",
    "    y = np.exp(y)\n",
    "    y_tr = np.exp(y_tr)\n",
    "    y_va = np.exp(y_va)\n",
    "\n",
    "    _y = np.exp(_y)\n",
    "    _y_va = np.exp(_y_va)\n",
    "    _y_tr = np.exp(_y_tr)\n",
    "    \n",
    "    #predicted\n",
    "    _Y = pd.Series(np.ravel(_y))\n",
    "    _Y_TR = pd.Series(np.ravel(_y_tr))\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "#     _Y_TE = pd.Series(np.ravel(_y_te))\n",
    "    \n",
    "    #original\n",
    "    Y =  pd.Series(np.ravel(y))\n",
    "    Y_TR = pd.Series(np.ravel(y_tr))\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "#     Y_TE = pd.Series(np.ravel(y_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    print('Total RMSE :', math.sqrt(mean_squared_error(_Y, Y)))\n",
    "    print('Training RMSE :', math.sqrt(mean_squared_error(_Y_TR, Y_TR)))\n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "#     print('Test RMSE :', math.sqrt(mean_squared_error(_Y_TE, Y_TE)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_VA[100:300], label='predicted')\n",
    "    plot_train, = plt.plot(Y_VA[100:300], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(mod_hist.history['loss'])\n",
    "plt.plot(mod_hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #recreate model \n",
    "    model = lstm.build_model([4, 7], [7, seq_len, state_neurons_1, state_neurons_1, output], dropouts, pre_train=file_name+\".h5\")\n",
    "    print(\"Created model and loaded weights from \"+file_name+\".h5\")\n",
    "    \n",
    "    # estimate accuracy on whole dataset using loaded weights\n",
    "    _y_val = lstm.predict_point_by_point_aux(model, [X_val_pm, X_val_ws, X_val_rh, X_val_bp, X_val_vws, X_val_sr, X_val_wd, X_val_temp])\n",
    "    \n",
    "    _y_val = _y_val.reshape(-1,1)\n",
    "    \n",
    "    y_val = y_val.reshape(-1,1)\n",
    "    \n",
    "    #inverse the predictions to its actual value\n",
    "    print(\"Predicted Output sample: \")\n",
    "    _y_va = scaler.inverse_transform(_y_val)\n",
    "    for i in range(5):\n",
    "        print(_y_va[i])\n",
    "    \n",
    "    #inverse the outputs to its actual value\n",
    "    print(\"Original Output sample: \")\n",
    "    y_va = scaler.inverse_transform(y_val)\n",
    "    for i in range(5):\n",
    "        print(y_va[i])    \n",
    "    \n",
    "    \n",
    "    y_va = np.exp(y_va)\n",
    "    _y_va = np.exp(_y_va)\n",
    "    \n",
    "    #predicted\n",
    "    _Y_VA = pd.Series(np.ravel(_y_va))\n",
    "    \n",
    "    #original\n",
    "    Y_VA = pd.Series(np.ravel(y_va))\n",
    "    \n",
    "    print('Validation RMSE :', math.sqrt(mean_squared_error(_Y_VA, Y_VA)))\n",
    "    \n",
    "    plot_predicted, = plt.plot(_Y_VA[100:300], label='predicted')\n",
    "    plot_train, = plt.plot(Y_VA[100:300], label='actual')\n",
    "    plt.legend(handles=[plot_predicted, plot_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(x):\n",
    "    return float(1.0 * x/(x+10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
